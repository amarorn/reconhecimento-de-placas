# ğŸ“š ReferÃªncia da API - Arquitetura Refatorada de VisÃ£o Computacional

## ğŸ“‹ **VISÃƒO GERAL**

Esta documentaÃ§Ã£o descreve a API completa da arquitetura refatorada de visÃ£o computacional para reconhecimento de placas de trÃ¢nsito e veÃ­culos.

## ğŸ—ï¸ **ESTRUTURA DA API**

### **ğŸ§© Componentes Principais**

```
vision/
â”œâ”€â”€ core/                    # Componentes principais
â”‚   â”œâ”€â”€ base_processor.py   # Interface base
â”‚   â””â”€â”€ vision_pipeline.py  # Pipeline principal
â”œâ”€â”€ preprocessing/           # PrÃ©-processamento de imagens
â”‚   â””â”€â”€ image_preprocessor.py
â”œâ”€â”€ detection/              # DetecÃ§Ã£o de objetos
â”‚   â””â”€â”€ yolo_detector.py
â”œâ”€â”€ ocr/                    # Reconhecimento de texto
â”‚   â””â”€â”€ text_extractor.py
â””â”€â”€ __init__.py             # InicializaÃ§Ã£o do mÃ³dulo

config/
â””â”€â”€ vision_architecture.py  # ConfiguraÃ§Ãµes centralizadas
```

## ğŸ”§ **CONFIGURAÃ‡ÃƒO**

### **âš™ï¸ ConfigPresets**

#### **Desenvolvimento**
```python
from config.vision_architecture import ConfigPresets

config = ConfigPresets.development()
# - YOLOv8n (CPU)
# - PaddleOCR (CPU)
# - Logging detalhado
# - Cache desabilitado
```

#### **ProduÃ§Ã£o**
```python
config = ConfigPresets.production()
# - YOLOv8x (GPU)
# - PaddleOCR (GPU)
# - Cache habilitado
# - Logging otimizado
```

#### **Edge Computing**
```python
config = ConfigPresets.edge()
# - YOLOv8n (CPU)
# - Tesseract (CPU)
# - Half precision
# - Cache otimizado
```

### **ğŸ”§ ConfiguraÃ§Ã£o Customizada**
```python
from config.vision_architecture import (
    VisionArchitectureConfig, 
    ModelConfig, 
    OCRConfig,
    PreprocessingConfig
)

config = VisionArchitectureConfig(
    detection_model=ModelConfig(
        name="yolov8x",
        confidence_threshold=0.7,
        device="cuda"
    ),
    ocr_model=OCRConfig(
        type="paddleocr",
        confidence_threshold=0.8,
        use_gpu=True
    ),
    preprocessing=PreprocessingConfig(
        type="ai_enhanced",
        denoising=True,
        contrast_enhancement=True
    )
)
```

## ğŸš€ **PIPELINE PRINCIPAL**

### **ğŸ”§ VisionPipeline**

#### **InicializaÃ§Ã£o**
```python
from vision.core.vision_pipeline import VisionPipeline

# Usar configuraÃ§Ã£o padrÃ£o
pipeline = VisionPipeline()

# Ou configuraÃ§Ã£o customizada
pipeline = VisionPipeline(config_dict)
```

#### **MÃ©todos Principais**

##### **process_image_advanced(image_path: str) â†’ PipelineResult**
Processa uma imagem com pipeline completo.

**ParÃ¢metros:**
- `image_path` (str): Caminho para a imagem

**Retorna:**
- `PipelineResult`: Resultado completo do processamento

**Exemplo:**
```python
result = pipeline.process_image_advanced("imagem.jpg")

if result.success:
    print(f"âœ… Processamento bem-sucedido em {result.processing_time:.2f}s")
    print(f"ğŸ” DetecÃ§Ãµes: {result.metadata['total_detections']}")
    print(f"ğŸ“ Textos: {result.metadata['total_texts']}")
    
    for final_result in result.final_results:
        detection = final_result['detection']
        text = final_result['primary_text']
        confidence = final_result['confidence_score']
        
        print(f"  - {detection['class_name']}: {text} (conf: {confidence:.3f})")
```

##### **process_batch(image_paths: List[str], output_dir: str = None) â†’ List[PipelineResult]**
Processa mÃºltiplas imagens em lote.

**ParÃ¢metros:**
- `image_paths` (List[str]): Lista de caminhos para imagens
- `output_dir` (str, opcional): DiretÃ³rio para salvar resultados

**Retorna:**
- `List[PipelineResult]`: Lista de resultados

**Exemplo:**
```python
image_paths = ["img1.jpg", "img2.jpg", "img3.jpg"]
results = pipeline.process_batch(image_paths, "output_dir")

successful = sum(1 for r in results if r.success)
print(f"âœ… Processadas {successful}/{len(results)} imagens com sucesso")
```

##### **get_pipeline_statistics() â†’ Dict[str, Any]**
Retorna estatÃ­sticas do pipeline.

**Retorna:**
- `Dict[str, Any]`: EstatÃ­sticas completas

**Exemplo:**
```python
stats = pipeline.get_pipeline_statistics()
print(f"VersÃ£o: {stats['pipeline_version']}")
print(f"Componentes: {stats['components']}")
print(f"Inicializado em: {stats['initialized_at']}")
```

##### **cleanup()**
Limpa recursos do pipeline.

**Exemplo:**
```python
pipeline.cleanup()
# Pipeline nÃ£o pode mais ser usado apÃ³s cleanup
```

## ğŸ“¸ **PRÃ‰-PROCESSAMENTO**

### **ğŸ”§ ImagePreprocessor**

#### **InicializaÃ§Ã£o**
```python
from vision.preprocessing.image_preprocessor import ImagePreprocessor

config = {
    'resize_enabled': True,
    'target_size': (640, 640),
    'denoising_enabled': True,
    'contrast_enhancement': True,
    'normalization': True
}

preprocessor = ImagePreprocessor(config)
```

#### **MÃ©todos Principais**

##### **preprocess(image: np.ndarray) â†’ PreprocessingResult**
Aplica pipeline completo de prÃ©-processamento.

**ParÃ¢metros:**
- `image` (np.ndarray): Imagem de entrada

**Retorna:**
- `PreprocessingResult`: Resultado do prÃ©-processamento

**Exemplo:**
```python
result = preprocessor.preprocess(image)
processed_image = result.processed_image
enhancements = result.enhancement_applied

print(f"Melhorias aplicadas: {enhancements}")
```

##### **resize_image(image: np.ndarray, target_size: Tuple[int, int]) â†’ np.ndarray**
Redimensiona imagem para tamanho especÃ­fico.

**ParÃ¢metros:**
- `image` (np.ndarray): Imagem de entrada
- `target_size` (Tuple[int, int]): Tamanho alvo (largura, altura)

**Retorna:**
- `np.ndarray`: Imagem redimensionada

##### **apply_denoising(image: np.ndarray) â†’ np.ndarray**
Aplica tÃ©cnicas de reduÃ§Ã£o de ruÃ­do.

**MÃ©todos disponÃ­veis:**
- `DenoisingMethod.GAUSSIAN`
- `DenoisingMethod.BILATERAL`
- `DenoisingMethod.MEDIAN`
- `DenoisingMethod.NON_LOCAL_MEANS`

##### **enhance_contrast(image: np.ndarray) â†’ np.ndarray**
Melhora o contraste da imagem.

**MÃ©todos disponÃ­veis:**
- `EnhancementMethod.HISTOGRAM_EQUALIZATION`
- `EnhancementMethod.CLAHE`
- `EnhancementMethod.ADAPTIVE_THRESHOLD`

##### **get_preprocessing_summary() â†’ Dict[str, Any]**
Retorna resumo das configuraÃ§Ãµes.

## ğŸ” **DETECÃ‡ÃƒO**

### **ğŸ”§ YOLODetector**

#### **InicializaÃ§Ã£o**
```python
from vision.detection.yolo_detector import YOLODetector

config = {
    'weights_path': 'yolov8n.pt',
    'confidence_threshold': 0.5,
    'nms_threshold': 0.4,
    'input_size': (640, 640),
    'device': 'auto'
}

detector = YOLODetector(config)
```

#### **MÃ©todos Principais**

##### **detect(image: np.ndarray) â†’ DetectionBatchResult**
Executa detecÃ§Ã£o na imagem.

**ParÃ¢metros:**
- `image` (np.ndarray): Imagem de entrada

**Retorna:**
- `DetectionBatchResult`: Resultado da detecÃ§Ã£o

**Exemplo:**
```python
result = detector.detect(image)
print(f"Detectados {len(result.detections)} objetos")

for detection in result.detections:
    print(f"  - {detection.class_name}: {detection.confidence:.3f}")
    print(f"    BBox: {detection.bbox}")
    print(f"    Ãrea: {detection.area}")
```

##### **detect_traffic_signs(image: np.ndarray) â†’ List[DetectionResult]**
Detecta especificamente placas de trÃ¢nsito.

##### **detect_vehicle_plates(image: np.ndarray) â†’ List[DetectionResult]**
Detecta especificamente placas de veÃ­culos.

##### **detect_vehicles(image: np.ndarray) â†’ List[DetectionResult]**
Detecta especificamente veÃ­culos.

##### **filter_detections_by_size(detections: List[DetectionResult], min_area: float, max_area: float) â†’ List[DetectionResult]**
Filtra detecÃ§Ãµes por tamanho.

##### **filter_detections_by_confidence(detections: List[DetectionResult], min_confidence: float) â†’ List[DetectionResult]**
Filtra detecÃ§Ãµes por confianÃ§a.

##### **get_detection_statistics(detections: List[DetectionResult]) â†’ Dict[str, Any]**
Retorna estatÃ­sticas das detecÃ§Ãµes.

##### **draw_detections(image: np.ndarray, detections: List[DetectionResult], show_labels: bool = True, show_confidence: bool = True) â†’ np.ndarray**
Desenha detecÃ§Ãµes na imagem.

## ğŸ“ **OCR**

### **ğŸ”§ TextExtractor**

#### **InicializaÃ§Ã£o**
```python
from vision.ocr.text_extractor import TextExtractor, OCRType

config = {
    'type': OCRType.PADDLEOCR,
    'language': 'pt',
    'confidence_threshold': 0.7,
    'use_gpu': False,
    'apply_plate_rules': True
}

extractor = TextExtractor(config)
```

#### **MÃ©todos Principais**

##### **extract_text(image: np.ndarray, regions: List[Dict[str, Any]] = None) â†’ OCRBatchResult**
Extrai texto da imagem ou regiÃµes especÃ­ficas.

**ParÃ¢metros:**
- `image` (np.ndarray): Imagem de entrada
- `regions` (List[Dict[str, Any]], opcional): RegiÃµes para processar

**Retorna:**
- `OCRBatchResult`: Resultado da extraÃ§Ã£o de texto

**Exemplo:**
```python
# Extrair texto de toda a imagem
result = extractor.extract_text(image)

# Extrair texto de regiÃµes especÃ­ficas
regions = [{'bbox': (100, 100, 200, 150)}]
result = extractor.extract_text(image, regions)

for text_result in result.text_results:
    print(f"Texto: {text_result.text}")
    print(f"ConfianÃ§a: {text_result.confidence:.3f}")
    print(f"BBox: {text_result.bbox}")
```

##### **postprocess_text(text: str) â†’ str**
PÃ³s-processa texto extraÃ­do.

##### **get_ocr_statistics(results: OCRBatchResult) â†’ Dict[str, Any]**
Retorna estatÃ­sticas do OCR.

## ğŸ“Š **ESTRUTURAS DE DADOS**

### **ğŸ”§ ProcessingResult**
```python
@dataclass
class ProcessingResult:
    success: bool
    image_path: str
    processing_time: float
    detections: List[Dict[str, Any]]
    ocr_results: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    error_message: Optional[str] = None
    timestamp: datetime = None
```

### **ğŸ”§ PipelineResult**
```python
@dataclass
class PipelineResult:
    success: bool
    image_path: str
    processing_time: float
    preprocessing_result: Optional[Dict[str, Any]] = None
    detection_result: Optional[Dict[str, Any]] = None
    ocr_result: Optional[Dict[str, Any]] = None
    final_results: List[Dict[str, Any]] = None
    metadata: Dict[str, Any] = None
    error_message: Optional[str] = None
    timestamp: datetime = None
```

### **ğŸ”§ DetectionResult**
```python
@dataclass
class DetectionResult:
    bbox: Tuple[int, int, int, int]  # x, y, w, h
    confidence: float
    class_id: int
    class_name: str
    area: float
    center: Tuple[int, int]
```

### **ğŸ”§ TextResult**
```python
@dataclass
class TextResult:
    text: str
    confidence: float
    bbox: Tuple[int, int, int, int]  # x, y, w, h
    language: str
    processing_time: float
```

## ğŸ¯ **EXEMPLOS DE USO COMPLETOS**

### **ğŸš€ Exemplo BÃ¡sico**
```python
from config.vision_architecture import ConfigPresets
from vision.core.vision_pipeline import VisionPipeline

# ConfiguraÃ§Ã£o de desenvolvimento
config = ConfigPresets.development()
pipeline = VisionPipeline(config)

# Processar imagem
result = pipeline.process_image_advanced("imagem.jpg")

if result.success:
    print(f"âœ… Processamento concluÃ­do em {result.processing_time:.2f}s")
    
    for final_result in result.final_results:
        detection = final_result['detection']
        text = final_result['primary_text']
        confidence = final_result['confidence_score']
        
        print(f"ğŸ” {detection['class_name']}: {text} (conf: {confidence:.3f})")
else:
    print(f"âŒ Erro: {result.error_message}")

# Limpeza
pipeline.cleanup()
```

### **ğŸ”„ Exemplo de Processamento em Lote**
```python
from config.vision_architecture import ConfigPresets
from vision.core.vision_pipeline import VisionPipeline
from pathlib import Path

# ConfiguraÃ§Ã£o de produÃ§Ã£o
config = ConfigPresets.production()
pipeline = VisionPipeline(config)

# Encontrar todas as imagens em um diretÃ³rio
image_dir = Path("imagens")
image_paths = list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.png"))

# Processar em lote
results = pipeline.process_batch([str(p) for p in image_paths], "resultados")

# AnÃ¡lise dos resultados
successful = sum(1 for r in results if r.success)
total_time = sum(r.processing_time for r in results)
total_detections = sum(r.metadata['total_detections'] for r in results if r.success)

print(f"ğŸ“Š Resumo do Processamento:")
print(f"  âœ… Sucessos: {successful}/{len(results)}")
print(f"  â±ï¸  Tempo total: {total_time:.2f}s")
print(f"  ğŸ” Total de detecÃ§Ãµes: {total_detections}")

# Limpeza
pipeline.cleanup()
```

### **âš™ï¸ Exemplo com ConfiguraÃ§Ã£o Customizada**
```python
from config.vision_architecture import (
    VisionArchitectureConfig, 
    ModelConfig, 
    OCRConfig,
    PreprocessingConfig
)
from vision.core.vision_pipeline import VisionPipeline

# ConfiguraÃ§Ã£o personalizada para alta precisÃ£o
config = VisionArchitectureConfig(
    detection_model=ModelConfig(
        name="yolov8x",
        confidence_threshold=0.7,
        device="cuda",
        input_size=(1024, 1024)
    ),
    ocr_model=OCRConfig(
        type="paddleocr",
        confidence_threshold=0.85,
        use_gpu=True,
        language="pt"
    ),
    preprocessing=PreprocessingConfig(
        type="ai_enhanced",
        denoising=True,
        contrast_enhancement=True,
        normalization=True,
        additional_filters=True
    )
)

pipeline = VisionPipeline(config.__dict__)

# Processar com configuraÃ§Ã£o otimizada
result = pipeline.process_image_advanced("imagem_alta_qualidade.jpg")

# Limpeza
pipeline.cleanup()
```

## ğŸ§ª **TESTES**

### **ğŸ”¬ Executar Testes**
```bash
# Testes unitÃ¡rios
pytest tests/test_preprocessing.py -v
pytest tests/test_detection.py -v
pytest tests/test_ocr.py -v

# Testes de integraÃ§Ã£o
pytest tests/test_pipeline.py -v
pytest tests/test_integration.py -v

# Todos os testes com cobertura
pytest tests/ --cov=vision --cov=config --cov-report=html
```

### **ğŸ“Š Cobertura de Testes**
```bash
# Gerar relatÃ³rio HTML
pytest --cov=vision --cov=config --cov-report=html:htmlcov

# Abrir relatÃ³rio
open htmlcov/index.html
```

## ğŸš€ **DEPLOYMENT**

### **ğŸ³ Docker**
```dockerfile
FROM python:3.9-slim

# Instalar dependÃªncias do sistema
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    tesseract-ocr \
    tesseract-ocr-por

# Copiar cÃ³digo
COPY . /app
WORKDIR /app

# Instalar dependÃªncias Python
RUN pip install -r requirements_refatorado.txt

# Executar
CMD ["python", "examples/refactored_vision_example.py"]
```

### **â˜ï¸ Cloud Deployment**
```bash
# Google Cloud Run
gcloud run deploy vision-pipeline \
  --source . \
  --platform managed \
  --region us-central1 \
  --memory 4Gi \
  --cpu 2

# AWS Lambda
serverless deploy

# Azure Functions
func azure functionapp publish vision-pipeline
```

## ğŸ“ˆ **MONITORAMENTO E MÃ‰TRICAS**

### **ğŸ“Š EstatÃ­sticas do Pipeline**
```python
# Obter estatÃ­sticas completas
stats = pipeline.get_pipeline_statistics()

print(f"ğŸ“¦ VersÃ£o: {stats['pipeline_version']}")
print(f"ğŸ§© Componentes ativos: {stats['components']}")
print(f"ğŸ• Inicializado em: {stats['initialized_at']}")
print(f"ğŸ’¾ Cache: {stats['cache_size']} itens")
```

### **ğŸ” Logging Estruturado**
```python
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Logs automÃ¡ticos para cada componente
# - PrÃ©-processamento
# - DetecÃ§Ã£o
# - OCR
# - IntegraÃ§Ã£o
# - ValidaÃ§Ã£o
```

## ğŸš¨ **TRATAMENTO DE ERROS**

### **ğŸ”§ Tratamento de ExceÃ§Ãµes**
```python
try:
    result = pipeline.process_image_advanced("imagem.jpg")
    
    if result.success:
        print("âœ… Processamento bem-sucedido")
    else:
        print(f"âŒ Falha: {result.error_message}")
        
except Exception as e:
    print(f"ğŸ’¥ Erro inesperado: {e}")
    logging.error(f"Erro no pipeline: {e}", exc_info=True)
```

### **ğŸ”„ Fallback AutomÃ¡tico**
```python
# O sistema OCR tem fallback automÃ¡tico entre motores
# Se PaddleOCR falhar, tenta EasyOCR, depois Tesseract

# O detector YOLO tem fallback automÃ¡tico de dispositivo
# Se CUDA falhar, tenta CPU automaticamente
```

## ğŸ”® **ROADMAP FUTURO**

### **ğŸš€ VersÃ£o 2.1 (Q1 2025)**
- [ ] Suporte a DETR e EfficientDet
- [ ] Pipeline de fine-tuning automÃ¡tico
- [ ] API REST integrada
- [ ] Dashboard de monitoramento

### **ğŸš€ VersÃ£o 2.2 (Q2 2025)**
- [ ] Suporte a vÃ­deo em tempo real
- [ ] Modelos de segmentaÃ§Ã£o
- [ ] AnÃ¡lise de comportamento
- [ ] IntegraÃ§Ã£o com sistemas de trÃ¢nsito

### **ğŸŒŸ VersÃ£o 3.0 (Q4 2025)**
- [ ] Arquitetura distribuÃ­da
- [ ] AutoML para seleÃ§Ã£o de modelos
- [ ] Edge AI otimizado
- [ ] Suporte a mÃºltiplas lÃ­nguas

## ğŸ¤ **SUPORTE E CONTRIBUIÃ‡ÃƒO**

### **ğŸ“§ Suporte**
- **Issues**: [GitHub Issues]
- **DocumentaÃ§Ã£o**: [Wiki do Projeto]
- **Email**: [seu-email@exemplo.com]

### **ğŸ”§ ContribuiÃ§Ã£o**
1. Fork o projeto
2. Crie uma branch para sua feature
3. Implemente seguindo os padrÃµes da arquitetura
4. Adicione testes
5. Abra um Pull Request

### **ğŸ“‹ PadrÃµes de CÃ³digo**
- **Black**: FormataÃ§Ã£o automÃ¡tica
- **Flake8**: Linting
- **MyPy**: VerificaÃ§Ã£o de tipos
- **Docstrings**: DocumentaÃ§Ã£o obrigatÃ³ria

---

## ğŸ‰ **CONCLUSÃƒO**

Esta API fornece uma interface completa e flexÃ­vel para reconhecimento de placas de trÃ¢nsito e veÃ­culos, com:

âœ… **Modularidade**: Componentes independentes e intercambiÃ¡veis  
âœ… **Escalabilidade**: Suporte a diferentes cenÃ¡rios de uso  
âœ… **Testabilidade**: Arquitetura orientada a testes  
âœ… **Extensibilidade**: FÃ¡cil adiÃ§Ã£o de novas funcionalidades  
âœ… **Performance**: OtimizaÃ§Ãµes para diferentes hardwares  
âœ… **Manutenibilidade**: CÃ³digo limpo e bem documentado  

**ğŸš€ A API estÃ¡ pronta para uso em produÃ§Ã£o e desenvolvimento!**

---

*Ãšltima atualizaÃ§Ã£o: Janeiro 2025*